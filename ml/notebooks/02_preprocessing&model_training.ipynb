{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e081f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df049fbe",
   "metadata": {},
   "source": [
    "### Define X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735078fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop(columns=['EmployeeCount','StandardHours','Over18','EmployeeNumber','TrainingTimesLastYear','HourlyRate'])\n",
    "y=data['Attrition']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21860b",
   "metadata": {},
   "source": [
    "### Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3e9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ea1d7",
   "metadata": {},
   "source": [
    "### Pipeline construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a71d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, \n",
    "                             roc_curve, roc_auc_score, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# D√©finition des colonnes\n",
    "cat_cols = ['BusinessTravel', 'Department', 'JobRole', 'MaritalStatus', 'OverTime']\n",
    "num_cols = ['Age', 'DailyRate', 'DistanceFromHome', 'MonthlyIncome', 'MonthlyRate', \n",
    "            'NumCompaniesWorked', 'PercentSalaryHike', 'TotalWorkingYears', \n",
    "            'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', \n",
    "            'YearsWithCurrManager']\n",
    "\n",
    "# Preprocesseur\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ========== 1. D√âFINITION DES MOD√àLES ET GRILLES DE PARAM√àTRES ==========\n",
    "models_config = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'params': {\n",
    "            'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'model__penalty': ['l2'],\n",
    "            'model__solver': ['lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100, 200, 300],\n",
    "            'model__max_depth': [10, 20, 30, None],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'model__max_depth': [3, 5, 7],\n",
    "            'model__subsample': [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'params': {\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__kernel': ['rbf', 'linear'],\n",
    "            'model__gamma': ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ========== 2. ENTRA√éNEMENT AVEC GRIDSEARCHCV ==========\n",
    "def train_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Entra√Æne tous les mod√®les avec GridSearchCV\n",
    "    \"\"\"\n",
    "    trained_models = {}\n",
    "    \n",
    "    for model_name, config in models_config.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Entra√Ænement de {model_name}...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Cr√©er le pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', config['model'])\n",
    "        ])\n",
    "        \n",
    "        # GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=config['params'],\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Entra√Æner\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"\\nMeilleur score CV: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Meilleurs param√®tres: {grid_search.best_params_}\")\n",
    "        \n",
    "        trained_models[model_name] = grid_search\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "# ========== 3. MATRICE DE CONFUSION ==========\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Affiche la matrice de confusion\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Attrition', 'Attrition'],\n",
    "                yticklabels=['No Attrition', 'Attrition'])\n",
    "    plt.title(f'Matrice de Confusion - {model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Valeur R√©elle')\n",
    "    plt.xlabel('Valeur Pr√©dite')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# ========== 4. COURBE ROC ==========\n",
    "def plot_roc_curve(models_dict, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Affiche les courbes ROC pour tous les mod√®les\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for model_name, grid_search in models_dict.items():\n",
    "        # Pr√©dictions de probabilit√©\n",
    "        y_pred_proba = grid_search.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculer ROC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Tracer\n",
    "        plt.plot(fpr, tpr, linewidth=2, \n",
    "                label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "    \n",
    "    # Ligne diagonale\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Chance (AUC = 0.5)')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12)\n",
    "    plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=12)\n",
    "    plt.title('Courbes ROC - Comparaison des Mod√®les', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ========== 5. RAPPORT DE CLASSIFICATION ==========\n",
    "def print_classification_report(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Affiche le rapport de classification d√©taill√©\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RAPPORT DE CLASSIFICATION - {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                                target_names=['No Attrition', 'Attrition']))\n",
    "\n",
    "# ========== 6. TABLEAU COMPARATIF DES PERFORMANCES ==========\n",
    "def create_comparison_table(models_dict, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Cr√©e un tableau comparatif des performances\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name, grid_search in models_dict.items():\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        y_pred_proba = grid_search.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        results.append({\n",
    "            'Mod√®le': model_name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1-Score': f1_score(y_test, y_pred),\n",
    "            'ROC-AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "            'CV Score': grid_search.best_score_\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results.round(4)\n",
    "    df_results = df_results.sort_values('ROC-AUC', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TABLEAU COMPARATIF DES PERFORMANCES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# ========== 7. VISUALISATION COMPARATIVE ==========\n",
    "def plot_metrics_comparison(df_results):\n",
    "    \"\"\"\n",
    "    Graphique comparatif des m√©triques\n",
    "    \"\"\"\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx]\n",
    "        data = df_results.sort_values(metric, ascending=True)\n",
    "        \n",
    "        bars = ax.barh(data['Mod√®le'], data[metric], color='skyblue', edgecolor='navy')\n",
    "        \n",
    "        # Colorer la meilleure barre\n",
    "        max_idx = data[metric].idxmax()\n",
    "        bars[list(data.index).index(max_idx)].set_color('green')\n",
    "        \n",
    "        ax.set_xlabel('Score', fontsize=11)\n",
    "        ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Ajouter les valeurs sur les barres\n",
    "        for i, (idx, row) in enumerate(data.iterrows()):\n",
    "            ax.text(row[metric] + 0.01, i, f'{row[metric]:.3f}', \n",
    "                   va='center', fontsize=9)\n",
    "    \n",
    "    # Cacher le dernier subplot\n",
    "    axes[-1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ========== 8. FONCTION PRINCIPALE ==========\n",
    "def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fonction principale pour √©valuer tous les mod√®les\n",
    "    \"\"\"\n",
    "    # 1. Entra√Æner les mod√®les\n",
    "    print(\"\\nüöÄ PHASE 1: ENTRA√éNEMENT DES MOD√àLES AVEC GRIDSEARCHCV\")\n",
    "    trained_models = train_models(X_train, y_train)\n",
    "    \n",
    "    # 2. Tableau comparatif\n",
    "    print(\"\\nüìä PHASE 2: COMPARAISON DES PERFORMANCES\")\n",
    "    df_results = create_comparison_table(trained_models, X_test, y_test)\n",
    "    \n",
    "    # 3. Courbes ROC\n",
    "    print(\"\\nüìà PHASE 3: COURBES ROC\")\n",
    "    plot_roc_curve(trained_models, X_test, y_test)\n",
    "    \n",
    "    # 4. Graphiques comparatifs\n",
    "    print(\"\\nüìâ PHASE 4: VISUALISATIONS COMPARATIVES\")\n",
    "    plot_metrics_comparison(df_results)\n",
    "    \n",
    "    # 5. √âvaluation d√©taill√©e de chaque mod√®le\n",
    "    print(\"\\nüîç PHASE 5: √âVALUATION D√âTAILL√âE PAR MOD√àLE\")\n",
    "    for model_name, grid_search in trained_models.items():\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        plot_confusion_matrix(y_test, y_pred, model_name)\n",
    "        \n",
    "        # Rapport de classification\n",
    "        print_classification_report(y_test, y_pred, model_name)\n",
    "    \n",
    "    # 6. Meilleur mod√®le\n",
    "    best_model_name = df_results.iloc[0]['Mod√®le']\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    print(f\"\\nüèÜ MEILLEUR MOD√àLE: {best_model_name}\")\n",
    "    print(f\"ROC-AUC: {df_results.iloc[0]['ROC-AUC']:.4f}\")\n",
    "    \n",
    "    return trained_models, df_results, best_model\n",
    "\n",
    "# ========== EXEMPLE D'UTILISATION ==========\n",
    "\"\"\"\n",
    "# Charger vos donn√©es\n",
    "df = pd.read_csv('votre_fichier.csv')\n",
    "\n",
    "# S√©parer X et y\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df['Attrition']  # Assurez-vous que c'est binaire (0/1)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Lancer l'√©valuation compl√®te\n",
    "trained_models, results_df, best_model = evaluate_models(\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "# Faire des pr√©dictions avec le meilleur mod√®le\n",
    "predictions = best_model.predict(X_test)\n",
    "probabilities = best_model.predict_proba(X_test)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
